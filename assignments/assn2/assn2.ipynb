{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3db2a767"
   },
   "source": [
    "# Introductory Machine Learning: Assignment 2\n",
    "\n",
    "**Deadline:**\n",
    "\n",
    "Assignment 2 is due Thursday, October 5 at 11:59pm. Late work will not be accepted as per the course policies (see the Syllabus and Course policies on [Canvas](https://canvas.yale.edu).\n",
    "\n",
    "Sharing code or any written work is not okay, but discussing problems with the course staff or with other students is encouraged. Acknowledge any collaborations and source materials used.\n",
    "\n",
    "You should start early so that you have time to get help if you're stuck. The drop-in office hours schedule can be found on [Canvas](https://canvas.yale.edu).  You can also post questions or start discussions on [Ed Discussion](https://edstem.org/us/courses/44592/discussion/). The assignment may look long at first glance, but the problems are broken up into steps that should help you to make steady progress.\n",
    "\n",
    "**Submission:**\n",
    "\n",
    "Submit your assignment as a .pdf on Gradescope. You can access Gradescope through Canvas on the left-side of the class home page. The problems in each homework assignment are numbered. Note: When submitting on Gradescope, please select the correct pages of your pdf that correspond to each problem. This will allow graders to find your complete solution to each problem.\n",
    "\n",
    "To produce the .pdf, please do the following in order to preserve the cell structure of the notebook:  \n",
    "1.  Go to \"File\" at the top-left of your Jupyter Notebook\n",
    "2.  Under \"Download as\", select \"HTML (.html)\"\n",
    "3.  After the .html has downloaded, open it and then select \"File\" and \"Print\" (note you will not actually be printing)\n",
    "4.  From the print window, select the option to save as a .pdf\n",
    "\n",
    "**Topics**\n",
    "1. Logistic regression\n",
    "2. Regularization\n",
    "3. Stochastic gradient descent\n",
    "4. Trees\n",
    "5. Bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1f405d4"
   },
   "source": [
    "## Problem 1: Penguins: An ice example (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03c8e217"
   },
   "source": [
    "Can you tell one penguin species from another? Let's put what we've learned about logistic regression to (a very cute) test!\n",
    "\n",
    "<img src=\"https://github.com/YData123/sds265-fa23/raw/main/assignments/assn2/penguins_ds_species.png\" width=\"500\" align=\"center\" style=\"margin:10px 30px 10px 0px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75a4010e"
   },
   "source": [
    "We first need to learn a little about the anatomy of our flightless feathered friends. The *culmen* is the upper ridge of a birdâ€™s bill.\n",
    "\n",
    "<img src=\"https://github.com/YData123/sds265-fa23/raw/main/assignments/assn2/penguins_ds_culmen.png\" width=\"500\" align=\"center\" style=\"margin:10px 30px 10px 0px\">\n",
    "\n",
    "In our dataset, the culmen length and depth are called `bill_length_mm` and `bill_depth_mm`.\n",
    "\n",
    "*Data were collected and made available by [Dr. Kristen Gorman](https://www.uaf.edu/cfos/people/faculty/detail/kristen-gorman.php) and the [Palmer Station, Antarctica LTER](https://pallter.marine.rutgers.edu/), a member of the [Long Term Ecological Research Network](https://lternet.edu/). Artwork by @allison_horst.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "db83ab2a"
   },
   "outputs": [],
   "source": [
    "# run this cell to import needed packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5cf97aba"
   },
   "outputs": [],
   "source": [
    "# just run this cell to read in the data, and drop a couple variables\n",
    "\n",
    "df = pd.read_csv('https://github.com/YData123/sds265-fa22/raw/master/assignments/assn2/penguins.csv')\n",
    "df = df.drop(columns=['index','year','island'])\n",
    "df = df.dropna(axis=0)\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2425034",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Next we'll make the sex variable binary, and separate out\n",
    "# species as the label to predict\n",
    "\n",
    "species = list(set(df['species']))\n",
    "df['class'] = LabelEncoder().fit_transform(df['species'])\n",
    "sex = [int(list(df['sex'])[j]=='male') for j in range(len(df))]\n",
    "df['sex'] = sex\n",
    "df = df.drop(columns=['species'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5a9c6f46"
   },
   "source": [
    "### 1.1 Plotting the data\n",
    "\n",
    "The following cell shows a scatter plot of all pairs of variables. We showed this type of plot in class for the California housing data.\n",
    "Describe several entries in this plot, and what it tells you about the\n",
    "the relationship between the pairs of covariates. Do the relationships make intuitive sense? Why or why not? Do the data appear to have any obvious outliers or unusual patterns? Why or why not?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9f3bd7d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9d43975a"
   },
   "source": [
    "[Your markdown here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d98ae375"
   },
   "outputs": [],
   "source": [
    "def plot_features(df, feature1_name, feature2_name):\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    colors = ['orange', 'pink', 'lightgreen']\n",
    "    species = ['Adelie', 'Chinstrap', 'Gentoo']\n",
    "\n",
    "    for c in range(3):\n",
    "        mask = (df['class']==c)\n",
    "        plt.scatter(df[feature1_name][mask],\n",
    "                    df[feature2_name][mask],\n",
    "                    color=colors[c], label=species[c])\n",
    "\n",
    "    plt.xlabel(feature1_name)\n",
    "    plt.ylabel(feature2_name)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5c2fec00"
   },
   "source": [
    "### 1.1 Plotting the data (continued)\n",
    "\n",
    "Next, using the `plot_features` function above,\n",
    "give additional plots of pairs of features, shown together with\n",
    "the class labels. An example of such a plot is shown below:\n",
    "\n",
    "<img src=\"https://github.com/YData123/sds265-fa23/raw/main/assignments/assn2/feature_plot.jpg\" width=\"500\" align=\"center\" style=\"margin:10px 30px 10px 0px\">\n",
    "\n",
    "For each plot, discuss why the pair of features may or not be helpful in discriminating between the three species of penguins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4719837d"
   },
   "outputs": [],
   "source": [
    "# your code and markdown here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0edfb5a5"
   },
   "outputs": [],
   "source": [
    "# Just run the following cell, to get inputs X and labels y\n",
    "y = np.array(df['class'])\n",
    "X = df.copy()\n",
    "X = X.drop(columns=['class'])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4f36ab4"
   },
   "source": [
    "### 1.2 Standardize the data\n",
    "\n",
    "Next, standardize the data, so that each column of `X` has\n",
    "mean zero and standard deviation one. Note that after you have done this transformation, `X` should still be a `pandas.DataFrame`. Then, briefly explain why it is important to standardize the variables in the logistic regression models that you will train below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0be6b35a"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77e6afb8"
   },
   "source": [
    "[Your markdown here]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9e2981d6"
   },
   "source": [
    "### 1.3 Fit logistic regressions\n",
    "\n",
    "As done in the class demo for mushrooms and iris flowers, you will now  construct a series of logistic regression models using an increasing number of training points.\n",
    "\n",
    "Specifically, we want you to:\n",
    "\n",
    "* Vary the sample size from 10% of the data to 90% of the data, in increments of 10%\n",
    "* For each sample size, train a logistic regression model on randomly selected training points, and test on the remaining data\n",
    "* For each sample size, run 100 trials and average the error rates\n",
    "* Plot the resulting average error rates as a function of sample percentage of the data\n",
    "\n",
    "We will use the function sklearn.model_selection.train_test_split in each trial to randomly split the data into training and test sets.\n",
    "\n",
    "Here we repeat exactly what we did during class using `lr = LogisticRegression(solver='lbfgs', multi_class='multinomial')` to fit a logistic regression model to predict the three species  (*Adelie*, *Chinstrap*, and *Gentoo*).\n",
    "\n",
    "After filling in your code, simply run this cell to plot a learning curve of error rate as a function of training percentage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4cc9692",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='l2', C=.1, multi_class='multinomial')\n",
    "\n",
    "lr_error_rate = []\n",
    "trials = 100\n",
    "train_percent = np.linspace(.1, .9, 9)\n",
    "\n",
    "# For each training set percentage, create train/test split accordingly\n",
    "# and run logistic regression 100 times and calculate the average error rate\n",
    "\n",
    "\"\"\"\n",
    "function input:\n",
    "  X: data\n",
    "  y: label\n",
    "  ratio: train data percentage\n",
    "  trails: number of trails to run\n",
    "\n",
    "function return:\n",
    "  the average error rate with this train data percentage\n",
    "\"\"\"\n",
    "\n",
    "def get_err_rate(X, y, ratio, trials):\n",
    "    # todo:\n",
    "    # your code starts here\n",
    "    # your code ends here\n",
    "\n",
    "# Use get_err_rate to find the error rate for respective split size\n",
    "for p in train_percent:\n",
    "    lr_error_rate.append(get_err_rate(X, y, p, trials))\n",
    "\n",
    "\n",
    "plt.plot(train_percent, lr_error_rate)\n",
    "plt.xlabel('train percent')\n",
    "plt.ylabel('error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e71c953f"
   },
   "source": [
    "### 1.4 Choose good feature pairs\n",
    "\n",
    "Which pair of variables gives the best predictions of the species of penguins? Run some experiments and report back below!\n",
    "\n",
    "Note that there are ${5 \\choose 2} = 10$ ways you can choose a pair of features. You may want to run a loop to automate this :)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41d9929a"
   },
   "outputs": [],
   "source": [
    "# todo:\n",
    "# your code starts here\n",
    "# your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5f09eec"
   },
   "source": [
    "### 1.5 Visualize the decision boundries\n",
    "\n",
    "Using the two features you chose as the best, and at least two additional\n",
    "pairs of features, visualize the decision boundries of the logistic regression models.\n",
    "\n",
    "Is it what you expected? Where are you observing incorrect predictions? Describe why the decision boundaries make sense, from your understanding of the logistic regression model.\n",
    "\n",
    "Use the function `plot_decision_boundaries` that we supply below. (You may also modify this function to your liking.) An example plot is shown here:\n",
    "\n",
    "<img src=\"https://github.com/YData123/sds265-fa23/raw/main/assignments/assn2/sample_decision_boundaries.jpg\" width=\"400\" align=\"center\" style=\"margin:10px 30px 10px 0px\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3738b116"
   },
   "outputs": [],
   "source": [
    "def plot_decision_boundaries(X, y, model, error):\n",
    "    X2 = np.array(X)\n",
    "    b = model.intercept_\n",
    "    beta = model.coef_.T\n",
    "    colors = ['orange', 'pink', 'lightgreen']\n",
    "    h = 0.015\n",
    "    x_min, x_max = X2[:, 0].min() - .5, X2[:, 0].max() + .5\n",
    "    y_min, y_max = X2[:, 1].min() - .5, X2[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = np.dot(np.c_[xx.ravel(), yy.ravel()], beta) + b\n",
    "    Z = np.argmax(Z, axis=1)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    plt.contourf(xx, yy, Z, levels=[0,.5,1.5,2.5], colors=colors, alpha=0.5)\n",
    "    for c in range(3):\n",
    "        mask = (y==c)\n",
    "        plt.scatter(X2[np.array(mask),0], X2[np.array(mask),1], color=colors[c], label=species[c])\n",
    "\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title('error rate: %.2f' % error)\n",
    "    plt.xlabel(X.columns[0])\n",
    "    plt.ylabel(X.columns[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7344be6a"
   },
   "outputs": [],
   "source": [
    "# Your code and markdown here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d0657cc"
   },
   "source": [
    "### 1.6 (Optional) Use different regularization levels (2 points EC)\n",
    "\n",
    "Now that you have a working logistic regression algorithm, explore how to set the regularization level $C=1/\\lambda$. Use an appropriate cross-validation procedure to choose the regularization level, and explain your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ef99d0c6"
   },
   "outputs": [],
   "source": [
    "# Your code and markdown here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cf904fc5"
   },
   "source": [
    "Hooray! You are now a penguin master!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cb07baa0"
   },
   "source": [
    "## Problem 2: Mini-Batch Gradient Descent (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "877d5b2b"
   },
   "source": [
    "Consider a univariate logistic regression where we are trying to predict $Y$, which can take the value $0$ or $1$, from the variable $X$, which takes real values. Recall from lecture that we need to estimate parameters $\\beta_{0}$ and $\\beta_{1}$ by minimizing the penalized loss function:\n",
    "\n",
    "$L(\\beta_{0}, \\beta_{1}) = \\frac{1}{n}\\sum\\limits_{i=1}^{n} \\left[ log\\left( 1 + e^{\\beta_{0} + X_{i}\\beta_{1}}\\right) - Y_{i}\\left(\\beta_{0} + X_{i}\\beta_{1}\\right)\\right] + \\lambda \\beta_{1}^{2}$ .\n",
    "\n",
    "Note that generally the intercept is not penalized.\n",
    "\n",
    "In this problem, we will implement mini-batch gradient descent. At each iteration, a random set of $m$ samples from all $n$ samples is used to calculate the loss and gradient, which is the change in the loss with respect to the parameters. We then update the estimates of $\\beta_{0}$ and $\\beta_{1}$ based on the gradient.\n",
    "\n",
    "Run the next cell to simulate data using the parameter values of $\\beta_{0} = 3$ and $\\beta_{1} = -5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d48163ce"
   },
   "outputs": [],
   "source": [
    "n = 10000\n",
    "np.random.seed(265)\n",
    "x1 = np.random.uniform(-5, 5, size=n)\n",
    "beta0 = 3\n",
    "beta1 = -5\n",
    "p = np.exp(beta0 + x1*beta1)/(1 + np.exp(beta0 + beta1*x1))\n",
    "y = np.random.binomial(1, p, size=n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dc171d7"
   },
   "source": [
    "Here is a helper function for plotting we'll use later. Just run this cell; don't change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8e5cc69c"
   },
   "outputs": [],
   "source": [
    "def plot_betas_and_loss(beta0_all, beta1_all, loss_all, title=''):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18,5))\n",
    "    ax[0].plot(np.arange(len(beta0_all)), beta0_all)\n",
    "    ax[0].hlines(beta0, xmin=0, xmax=len(beta0_all),color = 'r')\n",
    "    ax[0].set_xlabel(\"Iteration\", fontsize=12)\n",
    "    ax[0].set_ylabel(r\"$\\widehat{\\beta}_{0}$\", fontsize=12)\n",
    "\n",
    "    ax[1].plot(np.arange(len(beta1_all)), beta1_all)\n",
    "    ax[1].hlines(beta1, xmin=0, xmax=len(beta1_all),color = 'r')\n",
    "    ax[1].set_xlabel(\"Iteration\", fontsize=12)\n",
    "    ax[1].set_ylabel(r\"$\\widehat{\\beta}_{1}$\", fontsize=12)\n",
    "    ax[1].set_title(title)\n",
    "\n",
    "    ax[2].plot(np.arange(len(loss_all)), loss_all)\n",
    "    ax[2].set_xlabel(\"Iteration\", fontsize=12)\n",
    "    ax[2].set_ylabel(\"Loss\", fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75cd0c42"
   },
   "source": [
    "### 2.1 Deriving the updates\n",
    "\n",
    "Let $L_{t}(\\beta_{0}, \\beta_{1})$ be the loss at $t$-th iteration.\n",
    "For given values of $\\beta_{0}$ and $\\beta_{1}$, the vector $\\left( \\dfrac{\\partial}{\\partial \\beta_{0}} L_{t}(\\beta_{0}, \\beta_{1}), \\dfrac{\\partial}{\\partial \\beta_{1}} L_{t}(\\beta_{0}, \\beta_{1}) \\right)^{T}$ is called the gradient of $L_{t}(\\beta_{0}, \\beta_{1})$ and is denoted $\\nabla L_{t}(\\beta_{0}, \\beta_{1})$.\n",
    "\n",
    "We calculate the derivative of $L_{t}(\\beta_{0}, \\beta_{1})$ with respect to $\\beta_{0}$, treating $\\beta_{1}$ as a constant. (i.e. calculate $\\dfrac{\\partial}{\\partial \\beta_{0}} L_{t}(\\beta_{0}, \\beta_{1})$):\n",
    "\n",
    "$\\dfrac{\\partial}{\\partial \\beta_{0}} L(\\beta_{0}, \\beta_{1}) = \\frac{1}{n}\\sum\\limits_{i=1}^{n} \\left(\\dfrac{e^{\\beta_{0} + X_{i}\\beta_{1}}}{1+e^{\\beta_{0} + X_{i}\\beta_{1}}}- Y_{i}\\right)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dfe4fb5"
   },
   "source": [
    "\n",
    "Now calculate the derivative of $L_{t}(\\beta_{0}, \\beta_{1})$ with respect to $\\beta_{1}$, treating $\\beta_{0}$ as a constant. (i.e. calculate $\\dfrac{\\partial}{\\partial \\beta_{1}} L_{t}(\\beta_{0}, \\beta_{1})$).\n",
    "\n",
    "Be sure to show your work by either typing it in here using LaTeX, or by taking a picture of your handwritten solutions and displaying them here in the notebook. (If you choose the latter of these two options, be sure that the display is large enough and legible. Please also upload a photo seperately to Gradescope in case the embedded image failed.)\n",
    "\n",
    "**[put your solution here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7d9d8d7"
   },
   "source": [
    "When we implement mini-batch stochastic gradient descent, we will use these formulas, but\n",
    "apply them to a mini-batch of $m$ randomly chosen datapoints, rather than to all $n$ datapoints\n",
    "(in our case $n=10,000$).\n",
    "Typically $m$ is chosen to be much, much smaller than $n$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c82e0a08"
   },
   "source": [
    "### 2.2 The key ingredients\n",
    "\n",
    "Complete the function `update` in the following cell which takes values for $\\beta_{0}$ and $\\beta_{1}$, a list `inds`  containing the indexes of the $m$ selected samples, as well as a step-size $\\eta$, and returns updated values for $\\beta_{0}$ and $\\beta_{1}$ from one step of gradient descent (using all the data and your answer to Part a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1a274c83"
   },
   "outputs": [],
   "source": [
    "def update(b0, b1, inds, step_size, lamb):\n",
    "\n",
    "    L_partial0 = ... # your implementation here, can be more than one line\n",
    "    L_partial1 = ... # your implementation here, can be more than one line\n",
    "\n",
    "    b0 -= step_size * L_partial0\n",
    "    b1 -= step_size * L_partial1\n",
    "    return (b0, b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a082e127"
   },
   "source": [
    "Now complete the function in the next cell called `loss` which takes values for $\\beta_{0}$ and $\\beta_{1}$,\n",
    "together with a subset of indices and regularization parameter, and returns the value of the loss function evaluated at those data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17daf255"
   },
   "outputs": [],
   "source": [
    "def loss(b0, b1, inds, lamb):\n",
    "\n",
    "    output = ... # your implementation here, can be more than one line\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59f3e7b7"
   },
   "source": [
    "### 2.3 Putting it all together\n",
    "\n",
    "Now complete the implementation of `minibatch_grad_descent` which puts all of these pieces together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "75e53c81"
   },
   "outputs": [],
   "source": [
    "def minibatch_grad_descent(b0=0, b1=0, batch_size=100, step_size=10, lamb=0, iterations=1000):\n",
    "    beta0_hat = b0\n",
    "    beta1_hat = b1\n",
    "    beta0_all = []\n",
    "    beta1_all = []\n",
    "    loss_all = []\n",
    "\n",
    "    for iter in range(iterations):\n",
    "        inds = ...  # your implementation: sample batch_size indices\n",
    "        batch_loss = ... # your implementation: call loss()\n",
    "        beta0_hat, beta1_hat = ... # your implementation: call update()\n",
    "\n",
    "        beta0_all.append(beta0_hat)\n",
    "        beta1_all.append(beta1_hat)\n",
    "        loss_all.append(batch_loss)\n",
    "        iter = iter+1\n",
    "\n",
    "    return (beta0_hat, beta1_hat, beta0_all, beta1_all, loss_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0da870f"
   },
   "source": [
    "Now, test your implementation by running the following cell, which will call the function\n",
    "with the default parameters, and then plot the beta parameters and loss during the course\n",
    "of stochastic gradient descent. We will check your plot as a first check that you have a correct implementation! You should expect the first plot looks like the following:\n",
    "\n",
    "<img src=\"https://github.com/YData123/sds265-fa23/raw/main/assignments/assn2/mini_batch_gd.png\" width=\"400\" align=\"center\" style=\"margin:10px 30px 10px 0px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a188e38b"
   },
   "outputs": [],
   "source": [
    "# Run this cell, don't change it!\n",
    "\n",
    "beta0_hat, beta1_hat, beta0_all, beta1_all, loss_all = minibatch_grad_descent()\n",
    "plot_betas_and_loss(beta0_all, beta1_all, loss_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1e8cfc5b"
   },
   "source": [
    "### 2.4 Assessing uncertainty\n",
    "\n",
    "We now use the code above that implements mini-batch gradient descent and\n",
    "run it several (30) times. We then display the mean and standard deviation of\n",
    "the estimates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0c33a827"
   },
   "outputs": [],
   "source": [
    "# run this cell, don't change it\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "beta0_hat_all_0 = []\n",
    "beta1_hat_all_0 = []\n",
    "for rep in tqdm(range(30)):\n",
    "    beta0_hat, beta1_hat, _, _, _ = minibatch_grad_descent()\n",
    "    beta0_hat_all_0.append(beta0_hat)\n",
    "    beta1_hat_all_0.append(beta1_hat)\n",
    "\n",
    "print('The mean of the estimated beta0 is %.2f' % np.mean(beta0_hat_all_0))\n",
    "print('The standard deviation of the estimated beta0 is %.3f' % np.std(beta0_hat_all_0))\n",
    "print('The mean of the estimated beta1 is %.2f' % np.mean(beta1_hat_all_0))\n",
    "print('The standard deviation of the estimated beta1 is %.3f' % np.std(beta1_hat_all_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8c23ae16"
   },
   "source": [
    "Comment on these results:\n",
    "\n",
    "1. Describe what causes the variation in the estimates.\n",
    "1. How would you construct approximate 95% confidence intervals for the estimates?\n",
    "1. Do the true parameters fall in those confidence intervals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb337ad9"
   },
   "source": [
    "**[Put your answers in this markdown cell]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea32a47d"
   },
   "source": [
    "### 2.5 Comparing regularization levels\n",
    "\n",
    "Repeat 2.3 with different $\\lambda$ (the regularization level), e.g. $\\lambda=0$, $\\lambda=.001$, and $\\lambda=.005$. Use (0,0) as the initial estimates of $\\beta_{0}$ and $\\beta_{1}$. Plot $\\beta_{0}$, $\\beta_{1}$, and $L(\\beta_{0}, \\beta_{1})$ vs. iteration number. How do the plots change as you change $\\lambda$? Are the changes consistent with your expectation? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9e6b0d8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run this cell, don't change it\n",
    "\n",
    "for lamb in [0, .001, .005]:\n",
    "    _, _, beta0_all, beta1_all, loss_all = minibatch_grad_descent(batch_size=500, lamb=lamb)\n",
    "    plot_betas_and_loss(beta0_all, beta1_all, loss_all, title = 'lambda=%.2e' % lamb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50bc55c0"
   },
   "source": [
    "Comment on these results:\n",
    "\n",
    "1. Describe what causes the differences in the plots across the three regularization levels.\n",
    "1. Does one of the three runs give a better estimate than the others? Why or why not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4600abcd"
   },
   "source": [
    "**[Put your answers in this markdown cell]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5480def"
   },
   "source": [
    "## Problem 3. Bias and Variance and Trees, Oh My! (20 points)\n",
    "\n",
    "\n",
    "In this problem you will explore the bias-variance tradeoff for decision trees using a simple toy regression problem. We start by importing a few packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7a690d38"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f334abfc"
   },
   "source": [
    "The following cell defines and plots the data for this regression problem. The true regression function is `f`; the response `y` is `f` plus noise. The true function is -1 above the line `x1==x2` and 1 below the line. Just run this cell; do not modify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0da3cc06"
   },
   "outputs": [],
   "source": [
    "n = 10000\n",
    "np.random.seed(265)\n",
    "\n",
    "X = np.random.uniform(low=-1, high=1, size=2*n).reshape(n,2)\n",
    "f = np.sign(X[:,0] - X[:,1]) + np.sign(X[:,0] + X[:,1])\n",
    "y = f + np.random.normal(size=n)\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "plt.scatter(X[:,0], X[:,1], c=f, alpha=.3, s=2.5, cmap='jet')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "_ = plt.title('True function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "865f57e2"
   },
   "source": [
    "### 3.1 Build regressions trees with different depth\n",
    "\n",
    "In this problem you are asked to build a sequence of regression trees using this data, to predict `y` from `x1` and `x2`, varying the tree depth.\n",
    "\n",
    "* Vary the maximuim tree depth from 1 to 7\n",
    "* Train each tree on a random set of 500 data points\n",
    "* Test on the remaining 10000 - 500 data points\n",
    "* Run 500 trials (train/test splits) for each depth.\n",
    "* Plot the MSE as a function of the maximum tree depth\n",
    "\n",
    "The cell below contains some starter code. You may modify this starter code in any way you wish. But be sure to *keep the lines at the end, which plots the mean squared error on the test data versus the depth.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e091c618"
   },
   "outputs": [],
   "source": [
    "trials = 500\n",
    "tree_depth = np.arange(1, 8)\n",
    "test_mean_squared_error = np.zeros(len(tree_depth))\n",
    "\n",
    "from tqdm import tqdm\n",
    "for d in tqdm(tree_depth):\n",
    "    rtree = DecisionTreeRegressor(max_depth=d)\n",
    "    for trial in np.arange(trials):\n",
    "        ...\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(tree_depth, test_mean_squared_error)\n",
    "plt.xlabel('tree depth')\n",
    "plt.ylabel('MSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4e2a072c"
   },
   "source": [
    "### 3.2 What is the best size of tree?\n",
    "\n",
    "(a) According to your plot above, what is the best tree depth to choose?\n",
    "\n",
    "(b) If the regression trees were trained on 5000 data points, rather than 500, how would the choice of tree depth change? Would it increase or decrease? Explain why. Try to answer this question without running any code! If you run an experiment, comment on this in your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29f0f9b6"
   },
   "source": [
    "[Your answer here in Markdown]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8be21d84"
   },
   "source": [
    "### 3.3 Estimate the squared bias and variance\n",
    "Now estimate the squared bias and variance of the trees as a function of the maximum depth. This is possible in this case because you know the true function `f`, which was defined above. Use the same setup as above:\n",
    "\n",
    "* Vary the maximuim tree depth from 1 to 7\n",
    "* Train each tree on a random set of 500 data points\n",
    "* Run 500 trials for each train/test split.\n",
    "\n",
    "To estimate the squared bias and variance, evaluate each of the models on each of the $n=10,000$ data points. You can then estimate the squared bias and variance of the predictions $\\hat y_i = \\hat f(x_i)$ and take the average\n",
    "over all the data points.\n",
    "\n",
    "Hint: Consider using the fact that the variance of a random variable can be written as $\\mbox{Var}(Z) = \\mathbb{E}(Z^2) - \\mathbb{E}(Z)^2$.  The cell below contains starter code based on this hint. You will need to use the true regression function `f` defined above as a numpy array.\n",
    "\n",
    "You may modify this starter code in any way you wish. But be sure to *keep the lines at the end, which plots the squared bias and variance.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ddb0497"
   },
   "outputs": [],
   "source": [
    "trials = 500\n",
    "tree_depth = np.arange(1, 8)\n",
    "bias_squared = []\n",
    "variance = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "for d in tqdm(tree_depth):\n",
    "    rtree = DecisionTreeRegressor(max_depth=d)\n",
    "    E_yhat = np.zeros(n)\n",
    "    E_yhat_squared = np.zeros(n)\n",
    "    for trial in np.arange(trials):\n",
    "        ...\n",
    "\n",
    "    # just run the following lines (or modify as needed)\n",
    "    bias_squared.append(np.mean((E_yhat - f)**2))\n",
    "    variance.append(np.mean(E_yhat_squared - E_yhat**2))\n",
    "\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(tree_depth, bias_squared, 'g-')\n",
    "ax2.plot(tree_depth, variance, 'r-')\n",
    "ax1.set_xlabel('tree depth')\n",
    "ax1.set_ylabel('squared bias', color='g')\n",
    "_ = ax2.set_ylabel('variance', color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e222f6a6"
   },
   "source": [
    "### 3.4 Do the bias and variance make sense?\n",
    "\n",
    "(a) Explain why your plots of the squared bias and variance make sense, and are consistent with your plot of the MSE computed previously.\n",
    "\n",
    "(b) What is the (approximate) value of the difference between the MSE and the sum of the squared bias and variance for this data set?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12873e70"
   },
   "source": [
    "[Your answer here in markdown]\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
