{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introductory Machine Learning: Assignment 3\n",
    "\n",
    "**Deadline:**\n",
    "\n",
    "Assignment 3 is due Thursday, October 26 at 11:59pm. Late work will not be accepted as per the course policies (see the Syllabus and Course policies on [Canvas](https://canvas.yale.edu).\n",
    "\n",
    "Directly sharing answers is not okay, but discussing problems with the course staff or with other students is encouraged.\n",
    "\n",
    "You should start early so that you have time to get help if you're stuck. The drop-in office hours schedule can be found on [Canvas](https://canvas.yale.edu).  You can also post questions or start discussions on [Ed Discussion](https://edstem.org/us/courses/9209/discussion/). The problems are broken up into steps that should help you to make steady progress.\n",
    "\n",
    "**Submission:**\n",
    "\n",
    "Submit your assignment as a .pdf on Gradescope, and as a .ipynb on Canvas. You can access Gradescope through Canvas on the left-side of the class home page. The problems in each homework assignment are numbered. Note: When submitting on Gradescope, please select the correct pages of your pdf that correspond to each problem. This will allow graders to find your complete solution to each problem.\n",
    "\n",
    "To produce the .pdf, please do the following in order to preserve the cell structure of the notebook:  \n",
    "1.  Go to \"File\" at the top-left of your Jupyter Notebook\n",
    "2.  Under \"Download as\", select \"HTML (.html)\"\n",
    "3.  After the .html has downloaded, open it and then select \"File\" and \"Print\" (note you will not actually be printing)\n",
    "4.  From the print window, select the option to save as a .pdf\n",
    "\n",
    "**Topics**\n",
    "1. Random forests\n",
    "2. Principal components analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Seeing the random forests for the trees (25 points)\n",
    "\n",
    "This problem is based on the `diabetes` dataset from the `sklearn` package. Please read about the dataset at [https://scikit-learn.org/stable/datasets/index.html#diabetes-dataset](https://scikit-learn.org/stable/datasets/index.html#diabetes-dataset). We will predict the response, which is a quantitative measure of diabetes progression one year after baseline, using regression trees and random forests.\n",
    "\n",
    "The following cell imports the dataset as `diabetes` and names the predictor variables `diabetes_x` and the response `diabetes_y`. Just run this cell, do not modify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "diabetes = datasets.load_diabetes()\n",
    "diabetes_x = diabetes.data\n",
    "diabetes_y = diabetes.target\n",
    "print(diabetes.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Building a simple regression tree\n",
    "\n",
    "To start, you will *manually* build a regression tree using only two of the predictor variables: `bmi` and `s5`. To keep things simple, build a tree that has exactly three nodes and four leaves. So, the data is split into two parts initially and then each of those parts is again split one more time. At each node you will need to evaluate each possible splitting point for both `bmi` and `s5` and pick the one that minimizes the RSS.\n",
    "\n",
    "When you have built the regression tree, create a scatter plot of `s5` versus `bmi`, color-coded by the response variable. In this plot, use vertical and horizontal lines to indicate the regions that your tree splits the data into. You may find the functions `plt.hlines()` and `plt.vlines()` to be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- please write your answer here. -- ## \n",
    "# extract the data\n",
    "bmi = diabetes_x[:,2]\n",
    "s5 = diabetes_x[:,8]\n",
    "rss_bmi = []\n",
    "rss_s5 = []\n",
    "\n",
    "# we put a wrapper on the np.mean function to avoid warnings from taking the average of an empty list\n",
    "def average(x):\n",
    "    if len(x) == 0:\n",
    "        return(0.0)\n",
    "    else:\n",
    "        return(np.mean(x))\n",
    "\n",
    "# the following starter code finds the best splits for bmi and bp at the root\n",
    "for i in range(len(bmi)):\n",
    "    left = np.where(bmi <= bmi[i])[0]\n",
    "    right = np.where(bmi > bmi[i])[0]\n",
    "    rss_bmi.append(np.sum((diabetes_y[left] - average(diabetes_y[left]))**2) + \n",
    "                   np.sum((diabetes_y[right] - average(diabetes_y[right]))**2))\n",
    "    left = np.where(s5 <= s5[i])[0]\n",
    "    right = np.where(s5 > s5[i])[0]\n",
    "    rss_s5.append(np.sum((diabetes_y[left] - average(diabetes_y[left]))**2) + \n",
    "                  np.sum((diabetes_y[right] - average(diabetes_y[right]))**2))\n",
    "    \n",
    "best_bmi_cut = np.argmin(rss_bmi)\n",
    "best_s5_cut = np.argmin(rss_s5)\n",
    "\n",
    "# You should feel free to rewrite the above code in any way that suits you.\n",
    "# Now complete the code to make the best split, and then split each child node, \n",
    "# and then visualize the tree, showing the four leaf rectangles in the space s5 vs. bmi \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some starter code that you can use to show the four rectangles defined by the leaves. Modify this to use the regions defined by the decision tree above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(bmi, s5, c = diabetes_y)\n",
    "plt.ylabel(\"S5\", fontsize=14)\n",
    "plt.xlabel(\"BMI\", fontsize=14)\n",
    "plt.hlines(0, xmin=np.min(bmi), xmax=np.max(bmi), colors='r')\n",
    "plt.vlines(0, ymin=np.min(s5), ymax=np.max(bmi), colors='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Fitting a full regression tree\n",
    "\n",
    "Now build a tree that uses all the predictor variables, has a more flexible structure, and is validated with a test set. Split the full dataset into a training set and a test set of equal size (50/50). Fit a regression tree to the training set using the function `DecisionTreeRegressor` from `sklearn.tree`. For now, use your best judgment to choose parameters for tree complexity; we will use cross-validation to choose parameters in later parts of this problem set. Some starter code is provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# regr = tree.DecisionTreeRegressor().fit()\n",
    "# tree parameters go inside the first set of parentheses and the\n",
    "# training data goes in the second set of parentheses. Check the\n",
    "# documentation for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Plotting the tree\n",
    "\n",
    "Plot your regression tree. To do so, just execute the cell below; no\n",
    "need to modify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(regr, filled=True, feature_names=diabetes.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Evaluation\n",
    "\n",
    "Interpret your regression tree. What are some examples of variables that seem to correspond to higher or lower measures of diabetes progression? What is the MSE of the model using the test set? The `.predict` method for your model can help with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Using random forests\n",
    "\n",
    "Finally, we will grow random forests to analyze the data,\n",
    "using the `RandomForestRegressor` function from `sklearn.ensemble`. Again, please use your best judgment to choose the initial parameters for tree complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "# Here is an example of how to use the random forest function in sklearn.ensemble.\n",
    "# The code below assumes that the training inputs and responses are loaded in the variables train_x and train_y\n",
    "# and that the test predictor variables are in test_x\n",
    "\n",
    "# dtr = ensemble.RandomForestRegressor(min_samples_leaf=15, max_features=m)\n",
    "# regr = dtr.fit(train_x, train_y)\n",
    "# pred_y = regr.predict(test_x)\n",
    "# mse = np.mean(np.square(test_y-pred_y))\n",
    "\n",
    "## -- please write your answer here. -- ## \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions to compare the performance of random forests to a single regression tree.\n",
    "\n",
    "1. What test MSE do you obtain, and how does it compare to the test MSE of the regression tree above? \n",
    "\n",
    "1. According to the model, which variables are most important in predicting diabetes progression? (The `.feature_importances_` method of the model may help with this.)\n",
    "\n",
    "1. Plot the MSE of the prediction against $m$, the number of variables considered at each split.\n",
    "\n",
    "1. Comment on the plot you created and if it makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your answers here, using a mix of Markdown and code, as appropriate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: PCA: Penguin Culmen Analysis (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's revisit our flightless friends with the new tools we've learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell to import needed packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell to read in the data\n",
    "\n",
    "df = pd.read_csv('https://github.com/YData123/sds265-fa22/raw/master/assignments/assn2/penguins.csv')\n",
    "df = df.drop(columns=['index','year','island'])\n",
    "df = df.dropna(axis=0)\n",
    "\n",
    "# encode the labels\n",
    "species = list(set(df['species']))\n",
    "df['class'] = LabelEncoder().fit_transform(df['species'])\n",
    "sex = [int(list(df['sex'])[j]=='male') for j in range(len(df))]\n",
    "df['sex'] = sex\n",
    "df = df.drop(columns=['species'])\n",
    "\n",
    "y = np.array(df['class'])\n",
    "X = df.copy()\n",
    "X = X.drop(columns=['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell to standardize\n",
    "\n",
    "for i in range(5):\n",
    "    X.iloc[:,i] = (X.iloc[:,i]-np.mean(X.iloc[:,i]))/np.std(X.iloc[:,i])\n",
    "\n",
    "print(f\"X is {X.shape[0]} rows with {X.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Run PCA\n",
    "\n",
    "In the next cell, carry out Principle Component Analysis to reduce the data from 5 dimensions to 2.\n",
    "\n",
    "Let `pv1` be the first principal vector and let `pv2` be the second principal vector. Let `pcs` be the projection of the data onto the first two principal vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pv1 =  # your code here\n",
    "pv2 =  # your code here\n",
    "pcs =  # your code here\n",
    "\n",
    "principalX = pd.DataFrame(data = pcs, columns = ['PC1', 'PC2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Principle Component Analysis\n",
    "\n",
    "The next cell plots the principal vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "\n",
    "plt.bar(range(5), pv1)\n",
    "plt.xticks(range(5), X.columns, rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(range(5), pv2)\n",
    "plt.xticks(range(5), X.columns, rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the first two principle vectors orthogonal? Write code to check and also explain conceptually.\n",
    "\n",
    "Looking at the plot, what are the first two principle components capturing, in terms of the original features we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[your markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "\n",
    "plt.scatter(principalX.iloc[:, 0], principalX.iloc[:, 1], c = y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is being plotted in this cell above? If you are to add an xlabel and ylabel, what would you call them?\n",
    "\n",
    "Can you explain why there are pretty much 6 clusters in the plot?\n",
    "\n",
    "Recall when we plotted using two features of your choice in assignment 2. How are these different and similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[your markdoen here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Visualization with decision boundries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to add the decision boundries to the plot. (Similar to what we did in assignment 2, but this time using the first 2 principle components as x and y.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "# the function will again help you plot the decision boundries\n",
    "\n",
    "def plot_decision_boundaries(X, y, lr, error):\n",
    "    X2 = np.array(X)\n",
    "    b = lr.intercept_\n",
    "    beta = lr.coef_.T\n",
    "    colors = ['orange', 'pink', 'lightgreen']\n",
    "    h = 0.015\n",
    "    x_min, x_max = X2[:, 0].min() - .5, X2[:, 0].max() + .5\n",
    "    y_min, y_max = X2[:, 1].min() - .5, X2[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = np.dot(np.c_[xx.ravel(), yy.ravel()], beta) + b\n",
    "    Z = np.argmax(Z, axis=1)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    plt.contourf(xx, yy, Z, levels=[0,.5,1.5,2.5], colors=colors, alpha=0.5)\n",
    "    for c in range(3):\n",
    "        mask = (y==c)\n",
    "        plt.scatter(X2[np.array(mask),0], X2[np.array(mask),1], color=colors[c], label=species[c])\n",
    "\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title('error rate: %.2f' % error)\n",
    "    plt.xlabel(X.columns[0])\n",
    "    plt.ylabel(X.columns[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you think the model performed comparing to the models you ran in assignment 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: PCA Applications -- Faces and Genetics data\n",
    "\n",
    "In this problem, we will apply PCA to two applications. In the first, we will apply PCA to analyze some face image data and try to understand the variation in this data. In the second application, we will apply PCA to some genetics data and recover an interesting connection with geography!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3.1: Face image data\n",
    "\n",
    "In this problem, we will apply implementation of PCA to analyze face image data from the `olivetti_faces` dataset. The dataset consists of $64 \\times 64$ greyscale images of faces taken in a controlled setting, varying lighting, facial expressions (eyes open/closed, smiling/not smiling), and facial details (glass / no glasses). The dataset consists of 40 subjects with 10 images for each. It was collected by AT&T between 1992 and 1994.\n",
    "\n",
    "We will try to understand the variation in face (images) by performing principal component analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we fetch the dataset and define a utility function for plotting a gallery of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell fetches the data and defines a function to plot the images\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "\n",
    "# get data\n",
    "faces, _ = fetch_olivetti_faces(return_X_y=True)\n",
    "\n",
    "n_samples, n_features = faces.shape\n",
    "image_shape = (64, 64)\n",
    "\n",
    "# define a utility function to plot a gallery of images\n",
    "def plot_gallery(title, images, n_col=3, n_row=2, cmap=plt.cm.gray):\n",
    "    fig, axs = plt.subplots(\n",
    "        nrows=n_row,\n",
    "        ncols=n_col,\n",
    "        figsize=(2.0 * n_col, 2.3 * n_row),\n",
    "        facecolor=\"white\",\n",
    "        constrained_layout=True,\n",
    "    )\n",
    "    fig.set_constrained_layout_pads(w_pad=0.01, h_pad=0.02, hspace=0, wspace=0)\n",
    "    fig.set_edgecolor(\"black\")\n",
    "    fig.suptitle(title, size=16)\n",
    "    for ax, vec in zip(axs.flat, images):\n",
    "        im = ax.imshow(\n",
    "            vec.reshape(image_shape),\n",
    "            cmap=cmap,\n",
    "            interpolation=\"nearest\",\n",
    "        )\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    fig.colorbar(im, ax=axs, orientation=\"horizontal\", shrink=0.99, aspect=40, pad=0.01)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot first 6 images in dataset\n",
    "plot_gallery(\"First 6 images\", faces[:6, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.1.1: Fit PCA and visualize the top principal vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit PCA to the `faces` data. Use `n_components=None` to compute the maximal number of principal vectors\n",
    "# your code here\n",
    "pca = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the principal vectors from the fitted PCA object\n",
    "face_principal_vectors = ... # your code here\n",
    "plot_gallery(\"Our EigenFaces\", face_principal_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.1.2: Visualize explained variance of the top $k$ principal vectors\n",
    "\n",
    "When we compute PCA, we can get the \"explained variance\" of each principal vector. In this problem, we will visualize the explained variance to better-understand the variation in the data.\n",
    "\n",
    "Recall that in `sklearn`'s implementation of PCA, the explained variance ratio of the $i$ th principal vector is given by `pca.explained_variance_ratio_[i]`. This is a number between 0 and 1. You can think of this as the percentage of the variation in the data in the direction of the $i$ th principal vector.\n",
    "\n",
    "Then, we can define the cumulative explained variance of the top $k$ principal vectors as,\n",
    "$$\\mathrm{cumulative\\_explained\\_variance\\_ratio}(k) = \\sum_{i=1}^{k} \\mathrm{explained\\_variance\\_ratio}(i).$$\n",
    "\n",
    "This quantity represents the proportion of the variation in the data which is explained by the first $k$ principal vectors. In this problem, we will plot $k$ against against $\\mathrm{explained\\_variance\\_ratio}(k)$ and $\\mathrm{cumulative\\_explained\\_variance\\_ratio}(k)$. This gives us an idea of how many directions account for most of the variation in the data.\n",
    "\n",
    "------\n",
    "**Aside**: How is the \"explained variance ratio\" obtained?\n",
    "\n",
    "Recall that PCA is computed via the eigenvectors and eigenvalues of the empirical covariance matrix. Let $S$ be the empirical covariance matrix. Then, its eigenvectors $v_i$ are the principal vectors. Denote the eigenvalue of the $i$ th eigenvector by $\\lambda_i$. In `sklearn`, the eigenvalues are accessible via `pca.explained_variance_`; $\\lambda_i$ is `pca.explained_variance_[i]`. The \"explained variance ratio\" is a ratio of each eigenvalue to the sum of the eigenvalues. That is, $\\mathrm{explained\\_variance\\_ratio}(i) = \\lambda_i / \\sum_j \\lambda_j$.\n",
    "\n",
    "The eigenvalue of a particular eigenvector of the covariance matrix will be larger when there is more variation in that direction. The \"ratio\" part of explained variance simply normalizes these eigenvalues so that they sum to 1, giving the quantity an interpretation as the proportion of the variance explained by a particular direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the explained variance of the top k principal components as a function of k\n",
    "\n",
    "# extract the explained variance ratio of each principal vector\n",
    "explained_variance_ratio_per_component = ... # your code here\n",
    "\n",
    "# compute cumulative explained variance ratio upto k components for each k\n",
    "cumulative_explained_variance_ratio = ... # your code here (hint: you can use np.cumsum or do the sum yourself)\n",
    "\n",
    "# plot the explained variance ratio per component and the cumulative explained variance ratio\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(10, 5), ncols=2)\n",
    "ax1.plot(explained_variance_ratio_per_component)\n",
    "ax1.set_xlabel('component');\n",
    "ax1.set_ylabel('explained variance per component');\n",
    "\n",
    "\n",
    "ax2.plot(cumulative_explained_variance_ratio)\n",
    "ax2.set_xlabel('number of principal vectors');\n",
    "ax2.set_ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on the curve you observe. Roughly how many principal vectors are required to explain 90\\% of the variance? What does this tell you about the underlying variation in the data?\n",
    "\n",
    "[Your markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.1.3: Reconstructing data from principal components\n",
    "\n",
    "In this question, we will use PCA to re-construct the face images from fewer principal components. The plots above show that most of the variance in the data is explained by the first few principal components. In this question, we will visualize this by reconstructing a face image using the first $k$ principal components, varying $k$. We will then use the face image PCA to (attempt to) reconstruct an image of an octopus.\n",
    "\n",
    "Recall that the first $k$ principal components, $v_1, \\ldots, v_k$ give an orthonormal basis for a $k$-dimensional subspace of the data. The first $k$ principal components of a point $x \\in \\reals^d$ are given by $\\left(v_1^\\top (x - \\bar{x}), \\ldots, v_k^\\top (x - \\bar{x})\\right)$. Each principal component represents the amount of $x$ that lies in the direction of the corresponding principal vector. The principal components can then be used to reconstruct $x$ via,\n",
    "\n",
    "$$\\hat{x} = \\bar{x} + \\sum_{i=1}^k \\left(v_i^\\top (x - \\bar{x})\\right) v_i.$$\n",
    "\n",
    "This is essentially a projection onto the $k$-dimensional subspace spanned by the principal vectors $v_1, \\ldots, v_k$ (after centering by the mean).\n",
    "\n",
    "In the `sklearn` implementation of PCA, the mean vector $\\bar{x}$ is accessible via `pca.mean_` and the principal vectors are accessible by `pca.compnents_`.\n",
    "\n",
    "Implement a function `project_principal_vectors` which computes $\\hat{x}$ for a given $x$, reconstructing a point in terms of the principal vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_principal_vectors(x, principal_vecs, mean_x, k):\n",
    "    \"\"\"\n",
    "    project a data point onto the first k principal vectors\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray of shape (d,)\n",
    "        test data point\n",
    "    principal_vecs : np.ndarray of shape (min(d, n), d)\n",
    "        array of eigenvectors where eig_vecs[i] is the ith eigenvector\n",
    "    mean_x : np.ndarray of shape (d,)\n",
    "        the mean vector of the data\n",
    "    k : int\n",
    "        number of principal vectors to project onto\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray of shape (d,)\n",
    "        the projection of x onto the first k principal vectors\n",
    "    \"\"\"\n",
    "\n",
    "    # center the data point\n",
    "    centered_x = ... # your code here\n",
    "\n",
    "    # compute the projection onto the first k principal components\n",
    "    projection = mean_x + sum([\n",
    "        ... # your code here\n",
    "        for i in range(k)])\n",
    "\n",
    "    return projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract mean vector and principal components from fitted PCA object\n",
    "face_mean_x = ... # your code here\n",
    "face_principal_vectors = ... # your code here\n",
    "\n",
    "\n",
    "face = faces[0] # consider the first image in the dataset (you can change this to any image in the dataset)\n",
    "fig, axs = plt.subplots(figsize=(20,6), ncols=9)\n",
    "\n",
    "# plot the original image\n",
    "axs[0].imshow(face.reshape(image_shape), cmap='gray')\n",
    "axs[0].set_title('original')\n",
    "\n",
    "for rotated_ax, k in zip(axs[1:], [1, 5, 10, 50, 100, 200, 300, 400]):\n",
    "    # compute the reconstruction using the first k principal components using `project_principal_vectors`\n",
    "    reconstruction = ... # your code here\n",
    "\n",
    "    # plot the reconstructed image using the first k principal components\n",
    "    rotated_ax.imshow(reconstruction.reshape(image_shape), cmap='gray')\n",
    "    rotated_ax.set_title(f'k={k}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we try to apply our PCA algorithm, which was fitted on images of faces, to an image of something that is not a face? Let's try! Below, we'll attempt to use PCA to reconstruct an image of a cute octopus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source of image: \"https://i.pinimg.com/originals/6d/dc/5a/6ddc5a4845441a5d93583a8b2f8f562f.jpg\"\n",
    "# the cute_octopus.png file is a cropped, reshaped, and grayscale version of the image\n",
    "\n",
    "cute_octopus = plt.imread('https://raw.githubusercontent.com/YData123/sds265-fa23/main/assignments/assn3/cute_octopus.png')\n",
    "plt.imshow(cute_octopus, cmap='gray')\n",
    "cute_octopus = cute_octopus.flatten() # flatten the image into a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract mean vector and principal components from PCA object fitted to face data\n",
    "face_mean_x = ... # your code here\n",
    "face_principal_vectors = ... # your code here\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(20,6), ncols=9)\n",
    "\n",
    "# plot the original image\n",
    "axs[0].imshow(cute_octopus.reshape(image_shape), cmap='gray')\n",
    "axs[0].set_title('original')\n",
    "\n",
    "for rotated_ax, k in zip(axs[1:], [1, 5, 10, 50, 100, 200, 300, 400]):\n",
    "    # compute the reconstruction using the first k principal components using `project_principal_vectors`\n",
    "    reconstruction = ... # your code here\n",
    "\n",
    "    # plot the reconstructed image using the first k principal components\n",
    "    rotated_ax.imshow(reconstruction.reshape(image_shape), cmap='gray')\n",
    "    rotated_ax.set_title(f'k={k}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe about the reconstruction of the face image? How well do the reconstructed images match the original image at each $k$. What about the reconstruction of the octopus? Why do you think this is what we observe?\n",
    "\n",
    "[Your markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3.2: Analyzing genetics data\n",
    "\n",
    "In this sub-problem, we will PCA to analyzing genetics data. The data we will use comes from the [1000 Genomes Project](https://www.internationalgenome.org/).\n",
    "\n",
    "A single-nucleotide polymorphism is a substitution of a single nucleotide at a specific location in the genome which is present in a sufficiently large segment of the population. An ancestry-informative SNP (AISNP) is a SNP which has significant variation across global populations. There exists a line of work identifying AISNPs. In this problem, we will use 55 AISNPs identified by [Kidd et al.](https://pubmed.ncbi.nlm.nih.gov/24508742/). We have pre-processed this data for you.\n",
    "\n",
    "In this problem, you will use PCA to visualize and analyze this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the SNP data\n",
    "snp_data = pd.read_csv('https://raw.githubusercontent.com/YData123/sds265-fa23/main/assignments/assn3/1000genomes_snp_data.csv')\n",
    "snp_cols = [col for col in snp_data.columns if col.startswith('rs')]\n",
    "sample_attr_cols = ['pop', 'super_pop', 'gender']\n",
    "snp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode the SNP data using sklearn's OneHotEncoder for use with PCA\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "X = ohe.fit_transform(snp_data[snp_cols].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit PCA on the snp data in X; set the number of components to 2\n",
    "pca = ... # your code here\n",
    "\n",
    "# compute first two principal components of each sample in X\n",
    "snp_pcs = ... # your code here\n",
    "\n",
    "# add the principal components to the snp_data dataframe\n",
    "snp_data[['PC1', 'PC2']] = ... # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the principal components in a scatter plot with super population color-coded\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "sns.scatterplot(data=snp_data, x='PC1', y='PC2', hue='super_pop', alpha=0.5, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at this plot and think about any trends you see. Do you see any connection with geography?\n",
    "\n",
    "Below, we will rotate this plot so that the center of the EUR and AFR samples on the PC plot lie vertically on top of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean PC1 and PC2 for each super population\n",
    "mean_by_superpop = snp_data.groupby('super_pop')[['PC1', 'PC2']].mean()\n",
    "afr_pc_mean = mean_by_superpop.loc['AFR'].to_numpy()\n",
    "eur_pc_mean = mean_by_superpop.loc['EUR'].to_numpy()\n",
    "\n",
    "# compute angle between center of AFR and EUR clusters\n",
    "angle = np.arctan2(eur_pc_mean[1] - afr_pc_mean[1], eur_pc_mean[0] - afr_pc_mean[0])\n",
    "print(f'angle between AFR and EUR principal components: {angle:.2f} radians')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rotation_mat(angle):\n",
    "    '''creates 2D rotation matrix for given angle'''\n",
    "    c, s = np.cos(angle), np.sin(angle)\n",
    "    rotation_matrix = np.array(((c, -s), (s, c)))\n",
    "    return rotation_matrix\n",
    "\n",
    "# rotate the principal components by the angle between the AFR and EUR clusters\n",
    "rotation_mat = create_rotation_mat(angle - np.pi/2)\n",
    "rotated_pcs = snp_data[['PC1', 'PC2']].to_numpy() @ rotation_mat\n",
    "\n",
    "# create lines denoting the principal components in the rotated axes\n",
    "pc1_line = [(pc1, 0) for pc1 in ax.get_xlim()]\n",
    "pc2_line = [(0, pc2) for pc2 in ax.get_ylim()]\n",
    "pc1_line_rotated = pc1_line @ rotation_mat\n",
    "pc2_line_rotated = pc2_line @ rotation_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot rotated principal components\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "sns.scatterplot(x=rotated_pcs[:, 0], y=rotated_pcs[:, 1], hue=snp_data['super_pop'], alpha=0.5);\n",
    "ax.plot(pc1_line_rotated[:,0], pc1_line_rotated[:,1], color='black', linestyle='--', label='PC1');\n",
    "ax.plot(pc2_line_rotated[:,0], pc2_line_rotated[:,1], color='black', linestyle='--', label='PC2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, below is a cartogram of the global population from wikipedia. It shows a map of the world with land area scaled according to population.\n",
    "\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/9/90/Global_population_cartogram.png\" width=1000px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on any patterns you observe. Are \"super populations\" which are geographically closer also closer in terms of their first two principal components? Can you suggest any explanations for the patterns you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your markdown here]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
