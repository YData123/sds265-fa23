{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory Machine Learning: Assignment 5\n",
    "\n",
    "**Deadline:**\n",
    "\n",
    "Assignment 5 is due Thursday, November 30 at 11:59pm. Late work will not be accepted as per the course policies (see the Syllabus and Course policies on [Canvas](https://canvas.yale.edu).\n",
    "\n",
    "Directly sharing answers is not okay, but discussing problems with the course staff or with other students is encouraged.\n",
    "\n",
    "You should start early so that you have time to get help if you're stuck. The drop-in office hours schedule can be found on [Canvas](https://canvas.yale.edu).  You can also post questions or start discussions on [Ed Discussion](https://edstem.org/us/courses/44592/discussion). The problems are broken up into steps that should help you to make steady progress.\n",
    "\n",
    "**Submission:**\n",
    "\n",
    "Submit your assignment as a .pdf on Gradescope, and as a .ipynb on Canvas. You can access Gradescope through Canvas on the left-side of the class home page. The problems in each homework assignment are numbered. Note: When submitting on Gradescope, please select the correct pages of your pdf that correspond to each problem. This will allow graders to find your complete solution to each problem.\n",
    "\n",
    "To produce the .pdf, please do the following in order to preserve the cell structure of the notebook:  \n",
    "1.  Go to \"File\" at the top-left of your Jupyter Notebook\n",
    "2.  Under \"Download as\", select \"HTML (.html)\"\n",
    "3.  After the .html has downloaded, open it and then select \"File\" and \"Print\" (note you will not actually be printing)\n",
    "4.  From the print window, select the option to save as a .pdf\n",
    "\n",
    "**Topics**\n",
    "1. Bayesian inference\n",
    "2. Topic models\n",
    "3. Neural networks\n",
    "4. Reinforcement learning\n",
    "\n",
    "The first problem tests some of the basics of Bayesian inference and topic models. The second problem has you building topic models to improve pricing of houses on Zillow. The third problem gives you experience with neural networks in tensorflow. The fourth problem is a reinforcement learning task similar to the Taxi problem, but with a random environment. \n",
    "\n",
    "Note: The assignment looks longer than it really is. We step you through most of the code that you need. But it's still on the long side. Although the assignment is due in three weeks, we encourage you to start early!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1. Toy Story: Bayesian calculations (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gibbs sampling is one of the commonly used approach to approximate the inference for Latent Dirichlet Allocation model. In this problem, we will use the toy example from class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/YData123/sds265-fa23/main/assignments/assn5/diagram.png\" width=\"500\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that there are 3 documents and 15 words in the corpus. We would like to build a topic model with 3 topics. The proportions parameter is $\\alpha$ and the topic parameter is $\\eta$. The table below shows an assignment of topics to words in the toy corpus at one stage of the Gibbs sampling algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/YData123/sds265-fa23/main/assignments/assn5/words.png\" width=\"500\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only these assignment id $Z$ for each word, the following problems ask you \n",
    "to calculate the posterior topic proportions for each document, and word probabilities \n",
    "for one word in each of the three topics. To answer these questions you only need\n",
    "to use the basic properties of the Dirichlet distribution as a prior for \n",
    "a multinomial, as presented in class (and in the notes on Bayesian inference).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.1: Per-document topic proportions\n",
    "\n",
    "Given the $Z$ values in the table, what are the posterior distributions of $\\theta_{d}$ for documents $D_{1}$, $D_{2}$ and $D_{3}$ from left to right. Assume the prior over $\\theta$ is \n",
    "$\\mbox{Dirichlet}(\\alpha, \\alpha, \\alpha)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[your markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.2: Topics\n",
    "\n",
    "Here are the 15 words in our corpus:\n",
    "\n",
    "addiction, brother, baseball, catcher, daughter, divorce, drug, hit, inning, illegal, meth, mother, father, son, steroids\n",
    "\n",
    "What is the posterior mean for the probability $p(\\mbox{addiction} | \\mbox{topic 1})$? \n",
    "Assume that the prior distribution over the topics is $\\mbox{Dirichlet}(\\eta,...\\eta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[your markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the posterior mean of the probability $p(\\mbox{baseball}| \\mbox{topic 2})$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[your markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the posterior mean of the probability $p(\\mbox{divorce} | \\mbox{topic 3})$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[your markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2. Up: Rising house prices  (30 points)\n",
    "\n",
    "![zillow](https://raw.githubusercontent.com/YData123/sds265-fa23/main/assignments/assn5/zillow.png)\n",
    "\n",
    "### Overview of the problem\n",
    "\n",
    "Here we have a dataset of single family houses sold in Connecticut near the beginning of 2021, collected from [Zillow](https://www.zillow.com/homes/connecticut_rb/). You will build linear models of the price for which each house sold, based on its characteristics given in the real estate listing. Such characteristics include internal square footage, the year it was built, the bedroom count, the bathroom count, and the area of the lot. \n",
    "\n",
    "But there is also usually a lengthy description written by the real estate agent. Is there any additional information hidden in this description that would help improve the model of the price? This is the question we focus on in this problem.\n",
    "\n",
    "Answering such a question is difficult because the description is written in natural language with thousands of different words. Here we use topic models as a dimension reduction technique. Specifically, instead of using thousands of possible words, and how many times they show up in each house description, we reduce the words to the topic proportions $\\theta_d$ for each document, obtained by posterior inference. These proportions are combined with the other quantitative variables in a linear model with the logarithm of the house price as the response variable. \n",
    "\n",
    "*Important note:* At first glance, this problem looks really long. But this is deceiving. \n",
    "After reading in the data, we have you make some plots of the log-transformed variables. \n",
    "After that, you just need to run the code that leads up to training a 10-topic topic model, \n",
    "and fitting a linear model using the resulting topic proportions. After this, you are asked to compare the results to those obtained with a 3-topic model. To do this, you can simply copy the code used for the 10-topic model. After that, the crux of the problem is to analyze, understand, and describe the results.\n",
    "\n",
    "Acknowledgment: The data were scraped and the analysis was done by [Parker Holzer](https://parkerholzer.github.io/), as he began his search for a new house for his family after beginning a job as a data scientist. Thanks Parker!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:47.152805Z",
     "start_time": "2021-11-12T15:01:45.572491Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "from collections import Counter\n",
    "import statsmodels.formula.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in and clean up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:47.594652Z",
     "start_time": "2021-11-12T15:01:47.154274Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ct_homes = pd.read_csv('https://raw.githubusercontent.com/YData123/sds265-fa23/main/assignments/assn5/ct_zillow.csv')\n",
    "ct_homes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the data\n",
    "\n",
    "We add columns to `ct_homes` called `logAREA`, `logLOTSIZE`, and `logPRICE` that take the logarithms of the corresponding columns in the original data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:47.614588Z",
     "start_time": "2021-11-12T15:01:47.596230Z"
    }
   },
   "outputs": [],
   "source": [
    "ct_homes['logAREA'] = np.log(ct_homes['AREA'])\n",
    "ct_homes['logLOTSIZE'] = np.log(ct_homes['LOTSIZE'])\n",
    "ct_homes['logPRICE'] = np.log(ct_homes['PRICE'])\n",
    "ct_homes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Plot the data \n",
    "\n",
    "1. Show histograms of each of the log-transformed columns.\n",
    "\n",
    "1. Our regression models will use these transformed values. Why might it be preferable to use the logarithms rather than the original data? Explain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your code and markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one of the descriptions as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:47.620301Z",
     "start_time": "2021-11-12T15:01:47.617177Z"
    }
   },
   "outputs": [],
   "source": [
    "example = 9\n",
    "ct_homes[\"DESCRIPTION\"][example]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions\n",
    "\n",
    "The following two functions will be used to clean up the text a bit and separate into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:47.624584Z",
     "start_time": "2021-11-12T15:01:47.621530Z"
    }
   },
   "outputs": [],
   "source": [
    "def cleanup_description(desc):\n",
    "    if type(desc) == float:\n",
    "        desc = \"\"\n",
    "    words = [re.sub(r'[^a-z]', '', w) for w in desc.lower().split(' ')]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def reduce_to_vocabulary(desc, vocab):\n",
    "    return ' '.join([w for w in cleanup_description(desc).split(' ') if w in vocab])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:47.629948Z",
     "start_time": "2021-11-12T15:01:47.625889Z"
    }
   },
   "outputs": [],
   "source": [
    "cleanup_description(ct_homes['DESCRIPTION'][example])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Next we build a vocabulary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:47.919063Z",
     "start_time": "2021-11-12T15:01:47.631311Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = Counter()\n",
    "for dsc in ct_homes['DESCRIPTION']:\n",
    "    vocab.update(cleanup_description(dsc).split(' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:47.923544Z",
     "start_time": "2021-11-12T15:01:47.920814Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of unique tokens: %d\" % len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove words that are either too common or too rare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:48.081138Z",
     "start_time": "2021-11-12T15:01:47.925007Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = Counter(token for token in vocab.elements() if vocab[token] > 5)\n",
    "stop_words = [item[0] for item in vocab.most_common(50)]\n",
    "vocab = Counter(token for token in vocab.elements() if token not in stop_words)\n",
    "print(\"Number of unique tokens: %d\" % len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a mapping between unique words and integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:48.088577Z",
     "start_time": "2021-11-12T15:01:48.084325Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "desc = ct_homes['DESCRIPTION'][example]\n",
    "print('Original description:\\n---------------------')\n",
    "print(desc)\n",
    "\n",
    "print('\\nCleaned up text:\\n----------------')\n",
    "print(cleanup_description(desc))\n",
    "\n",
    "print('\\nReduced to vocabulary:\\n----------------------')\n",
    "print(reduce_to_vocabulary(desc, vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a mapping between unique words and integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:48.097985Z",
     "start_time": "2021-11-12T15:01:48.091685Z"
    }
   },
   "outputs": [],
   "source": [
    "id2word = {idx: pair[0] for idx, pair in enumerate(vocab.items())}\n",
    "word2id = {pair[0]: idx for idx, pair in enumerate(vocab.items())}\n",
    "\n",
    "s = 'nyc'\n",
    "print(\"Number of tokens mapped: %d\" % len(id2word))\n",
    "print(\"Identifier for '%s': %d\" % (s,word2id[s]))\n",
    "print(\"Word for identifier %d: %s\" % (word2id[s], id2word[word2id[s]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map to word id format\n",
    "\n",
    "Now, use the format required to build a language model, mapping each word to its id, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:48.656600Z",
     "start_time": "2021-11-12T15:01:48.101461Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for dsc in ct_homes['DESCRIPTION']:\n",
    "    clean = reduce_to_vocabulary(cleanup_description(dsc), vocab)\n",
    "    toks = clean.split(' ')\n",
    "    tokens.append(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:48.724420Z",
     "start_time": "2021-11-12T15:01:48.658177Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for toks in tokens:\n",
    "    tkn_count = Counter(toks)\n",
    "    corpus.append([(word2id[item[0]], item[1]) for item in tkn_count.items()])\n",
    "    \n",
    "dsc = ct_homes['DESCRIPTION'][example]\n",
    "clean = reduce_to_vocabulary(cleanup_description(dsc), vocab)\n",
    "toks = clean.split(' ')\n",
    "print(\"Abstract, tokenized:\\n\", toks, \"\\n\")\n",
    "print(\"Abstract, in corpus format:\\n\", corpus[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a Topic Model with 10 topics\n",
    "\n",
    "Note: Don't worry about the various settings used in the call to `LdaModel`. If you want to read up on these, just check out the documentation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:53.387268Z",
     "start_time": "2021-11-12T15:01:48.725885Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tm = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                     id2word=id2word,\n",
    "                                     num_topics=10, \n",
    "                                     random_state=100,\n",
    "                                     chunksize=100,\n",
    "                                     passes=10,\n",
    "                                     alpha='auto',\n",
    "                                     per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:53.405918Z",
     "start_time": "2021-11-12T15:01:53.388536Z"
    }
   },
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "num_words = 15\n",
    "top_words = pd.DataFrame({'word rank': np.arange(1,num_words+1)})\n",
    "for k in np.arange(num_topics): \n",
    "    topic = tm.get_topic_terms(k, num_words)\n",
    "    words = [id2word[topic[i][0]] for i in np.arange(num_words)]\n",
    "    probs = [topic[i][1] for i in np.arange(num_words)]\n",
    "    top_words['topic %d' % k] = words\n",
    "\n",
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:53.415623Z",
     "start_time": "2021-11-12T15:01:53.407114Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_dist = tm.get_document_topics(corpus[example])\n",
    "topics = [pair[0] for pair in topic_dist]\n",
    "probabilities = [pair[1] for pair in topic_dist]\n",
    "topic_dist_table = pd.DataFrame()\n",
    "topic_dist_table['Topic'] = topics\n",
    "topic_dist_table['Probabilities'] = probabilities\n",
    "topic_dist_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:53.704131Z",
     "start_time": "2021-11-12T15:01:53.417119Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(11,4)\n",
    "plt.bar(topic_dist_table['Topic'], topic_dist_table['Probabilities'], align='center', alpha=1, color='salmon')\n",
    "plt.xlabel('topic')\n",
    "plt.ylabel('probability')\n",
    "plt.title('Per Topic Probability Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include the topic proportions $\\theta_d$ for each house \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:56.326782Z",
     "start_time": "2021-11-12T15:01:53.705671Z"
    }
   },
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "theta = pd.DataFrame({\"Theta0\": np.zeros(ct_homes.shape[0])})\n",
    "for t in np.arange(1,num_topics):\n",
    "    theta[\"Theta\"+str(t)] = np.zeros(ct_homes.shape[0])\n",
    "    \n",
    "for i in np.arange(ct_homes.shape[0]):\n",
    "    for t in tm.get_document_topics(corpus[i]):\n",
    "        theta.loc[i,\"Theta\"+str(t[0])] = t[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:56.352081Z",
     "start_time": "2021-11-12T15:01:56.328006Z"
    }
   },
   "outputs": [],
   "source": [
    "ct_topics = ct_homes.join(theta)\n",
    "ct_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit a linear model with the topic proportions included\n",
    "\n",
    "We now fit a linear model with the topic proportions included. Note that \n",
    "the proportions satisfy $\\theta_0+\\theta_1+\\cdots + \\theta_9 = 1$. Therefore, we remove one of them, since it is redundant. If we don't do this the linear model will be harder to interpret!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:56.398437Z",
     "start_time": "2021-11-12T15:01:56.353552Z"
    }
   },
   "outputs": [],
   "source": [
    "model = sm.ols(\"logPRICE ~ logAREA + logLOTSIZE + BED + BATH + BUILT + Theta0 + \" +\n",
    "               \"Theta1 + Theta2 + Theta3 + Theta4 + Theta5 + Theta6 + Theta7 + Theta8\", data=ct_topics).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:56.604079Z",
     "start_time": "2021-11-12T15:01:56.399978Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(model.resid, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model without the topics included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:56.627406Z",
     "start_time": "2021-11-12T15:01:56.605673Z"
    }
   },
   "outputs": [],
   "source": [
    "model_without_topics = sm.ols(\"logPRICE ~ logAREA + logLOTSIZE + BED + BATH + BUILT\", data=ct_topics).fit()\n",
    "model_without_topics.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Plot the residuals\n",
    "\n",
    "On a single plot, show a histogram of the residuals of the model without the topics, \n",
    "and the residuals of the model with the topics. Give a legend that shows which is which.\n",
    "Comment on the results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:56.630729Z",
     "start_time": "2021-11-12T15:01:56.628696Z"
    }
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Quantify the improvement: R-squared\n",
    "\n",
    "How do the two models compare in terms of R-squared? What do these numbers mean?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[your markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Quantify the improvement: MSE decrease\n",
    "\n",
    "What is the percent decrease in the mean-squared-error of the model with the topics\n",
    "compared to the model that ignores the descriptions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:56.634159Z",
     "start_time": "2021-11-12T15:01:56.632262Z"
    }
   },
   "outputs": [],
   "source": [
    "# [your code and markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Quantify the improvement: LOOCV\n",
    "\n",
    "What is the percent decrease in the leave-one-out-cross-validation (LOOCV) error?\n",
    "Recall from class that the following formula can be used to calculate this:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/YData123/sds265-fa21/main/assignments/assn6/loocv.png\" width=\"410\" align=\"center\">\n",
    "\n",
    "<br>\n",
    "\n",
    "The following line of code computes this for one of the models:\n",
    "\n",
    "`np.mean((model.resid/(1 - model.get_influence().hat_matrix_diag))**2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:56.637660Z",
     "start_time": "2021-11-12T15:01:56.635497Z"
    }
   },
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Repeat for three topics \n",
    "\n",
    "Now, repeat the above steps for a topic model that is trained using only three (3) topics. Specifically:\n",
    "\n",
    "1. Train a model with three topics\n",
    "1. Display the top words in each of the three topics\n",
    "1. Augment the `ct_homes` data with the resulting topic proportions $\\theta$\n",
    "1. Fit a linear model *using only the first two of the three* proportions\n",
    "1. Plot a histogram of the residuals of the three linear models together\n",
    "1. Comment on the improvement over the baseline in terms of R-squared, MSE, and LOOCV compared with the previous two models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:56.641409Z",
     "start_time": "2021-11-12T15:01:56.639322Z"
    }
   },
   "outputs": [],
   "source": [
    "# your code and markdown here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7 Interpretation\n",
    "\n",
    "Now, interpret the model. Use the coefficients of the linear model to \n",
    "help interpret the meaning of the topics. Comment on what this says \n",
    "about the effectiveness of the topic model for predicting the sale price \n",
    "of the house. Does it make intuitive sense? Why or why not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[your markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Inside Out: Deep neural networks with tensorflow\n",
    "\n",
    "In class, we discussed a \"bare bones\" implementation of a 2-layer neural network for classification, using rectified linear units as activation functions. \n",
    "This implementation only uses the `numpy` package. In practice, we don't need to implement each component of a neural network from scratch. There exists software libraries like Tensorflow and PyTorch which make the process of building and training neural networks much faster. At their core, such libraries implement automatic differentiation so that the use only needs to define the \"forward pass\" of their model, and, as long as they do so within the framework, the gradients can be computed automatically. In addition, these libraries also have many built-in definitions of common neural network components. [Tensorflow](https://www.tensorflow.org/) is an open-source package for building and training neural networks. We will use it in this problem to explore applying neural networks to some synthetic classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that a 2-layer neural network for classification takes the following form,\n",
    "\\begin{aligned}\n",
    "    h_{1} &=\\operatorname{ReLU}\\left(W_{1} X+b_{1}\\right) \\\\\n",
    "    p &=\\operatorname{Softmax}\\left(W_{2} h_{1}+b_{2}\\right).\n",
    "\\end{aligned}\n",
    "where $X$ is the input. The number of layers is referred to as the \"depth\" of a neural network. We can construct neural networks with arbitrary depth via the following architecture,\n",
    "\\begin{aligned}\n",
    "    h_{1} &=\\operatorname{ReLU}\\left(W_{1} X+b_{1}\\right) \\\\\n",
    "    h_{2} &=\\operatorname{ReLU}\\left(W_{2} h_1 + b_{2}\\right) \\\\\n",
    "    h_{3} &=\\operatorname{ReLU}\\left(W_{3} h_2 + b_{3}\\right) \\\\\n",
    "    &\\vdots\\\\\n",
    "    p &=\\operatorname{Softmax}\\left(W_{\\ell} h_{\\ell-1}+b_{\\ell}\\right).\n",
    "\\end{aligned}\n",
    "This is an $\\ell$-layer neural network. The $i$-th layer takes the output of the previous layer, $h_{i-1}$ as input, and produces $h_i$. The parameters of the $i$-th layer are the weights matrix $W_i$ and the bias vector $b_i$. The nonlinear activation function is $\\operatorname{ReLU}$ at each hidden layer, and $\\operatorname{Softmax}$ at the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we will learn how to train deep neural networks with Tensorflow and apply them to synthetic classification tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T20:54:09.927009Z",
     "start_time": "2021-11-29T20:54:09.572756Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)  # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "# plt.rcParams['axes.facecolor'] = 'lightgray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we generate and plot a spiral dataset that contains $K=3$ classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T20:54:11.516809Z",
     "start_time": "2021-11-29T20:54:11.296972Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_spirals(N=100, K=3, noise=0.3):\n",
    "    D = 2  # dimensionality\n",
    "    X = np.zeros((N*K, D))\n",
    "    y = np.zeros(N*K, dtype='uint8')\n",
    "    for j in range(K):\n",
    "        ix = range(N*j, N*(j+1))\n",
    "        r = np.linspace(0.0, 1, N)  # radius\n",
    "        t = np.linspace(j*4, (j+1)*4, N) + np.random.randn(N)*noise  \n",
    "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "        y[ix] = j\n",
    "    return X, y\n",
    "\n",
    "def plot_data(X, y):\n",
    "    fig = plt.figure()\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "    plt.xlim(np.min(X[:,0])-.1, np.max(X[:,0])+.1)\n",
    "    plt.ylim(np.min(X[:,1])-.1, np.max(X[:,1])+.1)\n",
    "\n",
    "\n",
    "def plot_classifier(X, y, model):\n",
    "    h = 0.015\n",
    "    x_min, x_max = X[:, 0].min() - .2, X[:, 0].max() + .2\n",
    "    y_min, y_max = X[:, 1].min() - .2, X[:, 1].max() + .2\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    # Z = np.dot(np.maximum(\n",
    "    #     0, np.dot(np.c_[xx.ravel(), yy.ravel()], W1) + b1), W2) + b2\n",
    "    inputs = np.array([xx.ravel(), yy.ravel()]).T\n",
    "    Z = model(inputs)\n",
    "    Z = np.argmax(Z, axis=1)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    fig = plt.figure()\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.5)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    \n",
    "def plot_history(history, val=False, figsize=(8,3)):\n",
    "    '''plot given attributes from training history'''\n",
    "    plot_attrs = ('loss', 'accuracy')\n",
    "    fig, axs = plt.subplots(ncols=len(plot_attrs), figsize=figsize)\n",
    "\n",
    "    if not all(plot_attr in history.history for plot_attr in plot_attrs):\n",
    "        raise ValueError('not all `plot_attrs` are in the history object')\n",
    "\n",
    "    for plot_attr, ax in zip(plot_attrs, axs):\n",
    "        ax.plot(history.history[plot_attr], label=plot_attr)\n",
    "        if val:\n",
    "            ax.plot(history.history[f'val_{plot_attr}'], label=f'val_{plot_attr}')\n",
    "        ax.set_ylabel(plot_attr)\n",
    "        ax.set_xlabel('epoch')\n",
    "        ax.legend(loc='upper right')\n",
    "\n",
    "    return fig\n",
    "\n",
    "def train_neurnet(neurnet, X, y, X_val, y_val, n_epochs=1_000, batch_size=128):\n",
    "    neurnet.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    history = neurnet.fit(\n",
    "        X, y, validation_data=(X_val, y_val), epochs=n_epochs, batch_size=batch_size, verbose=0,\n",
    "        callbacks=[TqdmCallback(data_size = len(y), batch_size=batch_size, verbose=0)])\n",
    "    plot_history(history, val=True);\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T20:54:11.516809Z",
     "start_time": "2021-11-29T20:54:11.296972Z"
    }
   },
   "outputs": [],
   "source": [
    "X_spiral, y_spiral = generate_spirals(N=300, K=3)\n",
    "X_spiral_val, y_spiral_val = generate_spirals(N=50, K=3)\n",
    "X_spiral_test, y_spiral_test = generate_spirals(N=300, K=3)\n",
    "plot_data(X_spiral, y_spiral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train a 2-layer neural network to classify this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# demo of neurnet\n",
    "neurnet = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=3, activation='softmax')\n",
    "    ])\n",
    "\n",
    "neurnet.build(input_shape=(None, 2))\n",
    "neurnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = train_neurnet(neurnet, X_spiral, y_spiral, X_spiral_val, y_spiral_val, n_epochs=1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_classifier(X_spiral, y_spiral, neurnet)\n",
    "plot_classifier(X_spiral_test, y_spiral_test, neurnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.1: Experiment with different architectures and hyperparameters settings\n",
    "\n",
    "[Tensorflow playground](https://playground.tensorflow.org/) is a tool created by Tensorflow to demo how neural networks work, allowing users to tinker with different architectures on a few toy classification tasks (in fact, the same ones we are playing with in this problem). Under the hood, Tensorflow playground is running code like the one we are using in this notebook. Feel free to explore Tensorflow playground first before we continuing with this problem.\n",
    "\n",
    "Using the above code, experiment with different architectures and hyperparameters of the optimization. In particular, try the following two variations.\n",
    "1. Wider network. Increase the number of hidden units in the 2-layer network above. (hint: use the `units` argument in `layers.Dense`)\n",
    "2. Deeper network. Increase the depth of the network by adding an additional hidden layer. (hint: add an additional `layers.Dense`)\n",
    "\n",
    "For each variant, define your network using tensorflow, train it on the spiral data, visualize the decision boundaries, and include a markdown cell briefly describing what you observe. Feel free to add some additional variants as well. For example, you could train for longer, change the activation functions, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wider network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deeper network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.2: Apply to three other toy datasets\n",
    "\n",
    "Now, choose an architecture you like and apply your neural network to three more synthetic classification tasks generated by sklearn. For each of the following, define your neural network, train it on the dataset, and visualize the decision boundary. Similar to the above, feel free to experiment with different architectures to improve the performance of your neural network on each task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Circles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T21:29:50.361365Z",
     "start_time": "2021-11-29T21:29:50.237354Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# generate circles\n",
    "X_circles, y_circles = datasets.make_circles(300, noise=0.06, random_state=265)\n",
    "X_circles_val, y_circles_val = datasets.make_circles(100, noise=0.06, random_state=565)\n",
    "X_circles_test, y_circles_test = datasets.make_circles(300, noise=0.06, random_state=565)\n",
    "\n",
    "plot_data(X_circles, y_circles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first toy dataset forms two concentered circles with different radius, and we add a little bit of noise so that two circles start mixing together. Train a neural network on this taks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Blobs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T21:32:27.007059Z",
     "start_time": "2021-11-29T21:32:26.903357Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate blobs\n",
    "centers = np.array([[-.5,.0],[0,.5], [.5,.1], [-.2, -.7], [.2, -.3]])\n",
    "X_blobs, y_blobs = datasets.make_blobs(300, centers=centers, cluster_std=.15, center_box=(-1.0, 1.0), random_state=265)\n",
    "X_blobs_val, y_blobs_val = datasets.make_blobs(100, centers=centers, cluster_std=.15, center_box=(-1.0, 1.0), random_state=565)\n",
    "X_blobs_test, y_blobs_test = datasets.make_blobs(300, centers=centers, cluster_std=.15, center_box=(-1.0, 1.0), random_state=565)\n",
    "\n",
    "plot_data(X_blobs, y_blobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second dataset forms three clusters centered at different places. Repeat the procedure that you have walked through for the first dataset. Note that this time the number of classes is 5 not 3, so you'll have to adjust the number of units in the final unit accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T23:27:18.764555Z",
     "start_time": "2021-11-28T23:27:18.762744Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Moons\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T21:39:22.155382Z",
     "start_time": "2021-11-29T21:39:22.026240Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate moons\n",
    "X_moons, y_moons = datasets.make_moons(300, noise=0.2, random_state=265)\n",
    "X_moons_val, y_moons_val = datasets.make_moons(100, noise=0.2, random_state=565)\n",
    "X_moons_test, y_moons_test = datasets.make_moons(300, noise=0.2, random_state=565)\n",
    "\n",
    "plot_data(X_moons, y_moons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third dataset is composed of two moon-shaped clusters. Repeat the procedures above on this dataset. Note that the number of classes is 2 now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/5/53/Lake_Baikal_in_winter.jpg\" width=300 style=\"padding: 10px; float: right;\">\n",
    "\n",
    "\n",
    "## Problem 4: Frozen: Navigating a random environment (25 points)\n",
    "\n",
    "In class we introduce the Q-learning algorithm using the Taxi problem from the OpenAI `gym` package. In this problem we will explore another toy reinforcement learning problem, called  \"Frozen Lake\". In this problem you need to walk over a grid that represents a partially frozen lake, being careful not to fall through holes in the ice. (The author of this problem fell through the ice in a frozen lake when he was a kid&mdash;at night! It was quite an experience...) The environment is a simple 4x4 grid, and the goal is to walk from one corner to the other without falling through.\n",
    "\n",
    "Unlike the Taxi problem, but more like Tic-Tac-Toe as discussed in class, there are no intermediate rewards. Rather the reward is 1 if you succeed, and 0 otherwise. `Frozen Lake` has two versions. In the first version, the ice is (unrealistically) not slippery. Here the state transitions are deterministic: If you move right, you go right. In the second version, the state transitions are probabilistic: If you try to move right, you may go left, down, or up. Naturally, the slippery version is more challenging. \n",
    "\n",
    "Your task in this problem will be to complete the implementation of the Q-learning algorithm for this problem, display the value function, and then evaluate the solution. You'll do this for both the deterministic and random (slippery) versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load the necessary packages. You'll probably need to install `gym`, and can use `!pip install gym`. If you have difficulties let us know and we'll try to help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the \"ascii art\" rendition of the starting state. You start in the upper left corner, marked `S`, and the goal is to get to the lower right corner, marked `G`. The ice has four holes, marked `H`; if you step here you fall through the ice and the episode is done. But as you learn, you do not know where the holes are. We'll first use the deterministic version, by specifying `is_slippery=False`.\n",
    "\n",
    "This code was written and tested with `gym` version 0.26.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode='ansi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some simple helper functions, same as we used for the Taxi demo. Don't change the cell below, just run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(env, stat, action, reward):\n",
    "    return {'frame': env.render(),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward}\n",
    "\n",
    "\n",
    "def print_frames(frames, delay=.1):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(delay)\n",
    "        \n",
    "def display_value_function(q_table):\n",
    "    v = np.max(q_table, axis=1)\n",
    "    plt.imshow(v.reshape(4,4))\n",
    "    plt.axis('off')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    print(np.round(v.reshape(4,4), 3))\n",
    "\n",
    "def evaluate_Q_function(env, q_table, epsilon=.001, episodes=1000):\n",
    "    total_steps, total_successes = 0, 0\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        state, *_ = env.reset()\n",
    "        steps, reward = 0, 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "               action = env.action_space.sample() # Explore action space\n",
    "            else:\n",
    "                action = np.argmax(q_table[state]) # Exploit learned values\n",
    "    \n",
    "            next_state, reward, done, *_ = env.step(action)\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "\n",
    "        total_successes += reward\n",
    "        total_steps += steps\n",
    "    \n",
    "    print(f\"Results after {episodes} episodes:\")\n",
    "    print(f\"Average steps per episode: {total_steps / episodes}\")\n",
    "    print(f\"Chance of success: {total_successes / episodes}\")\n",
    "    \n",
    "def sample_episode(env, q_table, epsilon=.001):\n",
    "    state, *_ = env.reset()\n",
    "    steps, reward = 0, 0\n",
    "    done = False\n",
    "    frames = []\n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "        state, reward, done, *_ = env.step(action)\n",
    "        frames.append(render(env, state, action, reward))\n",
    "        steps += 1\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we simply walk around randomly, calling the `sample()` function to choose an action. At the end of the episode, the steps are displayed. If you're lucky, you'll make it to the goal; more likely is that you fall through the ice. Try it out a few times to make sure you understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "steps = 0\n",
    "reward = 0\n",
    "frames = [] \n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample() # choose a random action\n",
    "    state, reward, done, *_ = env.step(action)\n",
    "    frames.append(render(env, state, action, reward))\n",
    "    steps += 1\n",
    "    \n",
    "\n",
    "print_frames(frames, delay=.5)\n",
    "print(f\"\\nSteps taken: {steps}, success: {reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.1: Complete the Implementation of Q-learning\n",
    "\n",
    "In the cell below we have started you off with a partial implementation of Q-learning. Your job is to finish the implementation. You'll then use your implementation to learn the Q-function for both the deterministic and random versions of the environment. \n",
    "\n",
    "The function declaration looks like this:\n",
    "\n",
    "```python\n",
    "def Q_learning(env, alpha=.1, gamma=.7, epsilon=.1, episodes=10000):\n",
    "```\n",
    "\n",
    "with the following arguments:\n",
    "\n",
    "* `env` is the environment. We pass this in because we're going to have slippery and non-slippery versions.\n",
    "* `alpha` is the step size\n",
    "* `gamma` is the discount for future rewards\n",
    "* `epsilon` is the probability of exploring\n",
    "* `episodes` is the number of training episodes to use. \n",
    "\n",
    "Some hints:\n",
    "\n",
    "* The Q-table is intialized to have all values 1/2. When you transition to a state and the value of `done` is `True`, there are two possibilities: (1) You fell through a hole in the ice or (2) you reached the goal (congratulations!)\n",
    "\n",
    "* The value of the Q-function for these states (with all actions) should be zero; no future reward is possible.  You don't know where the holes in the ice are (even though you could \"cheat\" and read them off the rendering of the environment.\n",
    "\n",
    "* If you reached the goal, which is state 15, the value of the reward when you transition to this state will be 1.\n",
    "\n",
    "* All values of the Q-function should be less than or equal to 1---the value can be interpreted as the probability of reaching the goal starting from the state/action pair.\n",
    "\n",
    "* You should only need 3-5 lines of code to complete the implementation! If you find yourself using more, you may want to rethink your approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(env, alpha=.1, gamma=.7, epsilon=.1, episodes=10000):\n",
    "    q_table = 0.5*np.ones([env.observation_space.n, env.action_space.n])\n",
    "    for _ in tqdm(np.arange(episodes)):\n",
    "        state, *_ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            explore = False\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                explore = True\n",
    "                action = env.action_space.sample() # Explore action space\n",
    "            else:\n",
    "                action = np.argmax(q_table[state]) # Exploit learned values\n",
    "            next_state, reward, done, *_ = env.step(action) \n",
    "            ### Finish the implementation below. Only 3-5 lines of code needed.\n",
    "            \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.2\n",
    "\n",
    "Now use the function `Q_learning` that you just wrote, and explore different settings \n",
    "of the parameters. The following cells carry out the following steps:\n",
    "\n",
    "1. Create the environment\n",
    "1. Run Q-learning. This is where you can experiment with different parameters\n",
    "1. Display the value function `v(s) = np.max(q_table[s])`\n",
    "1. Describe the value function, and how the numerical values make sense\n",
    "1. Run `evaluate_Q_function` to get statistics on how well it works\n",
    "1. Comment on the evaluation statistics\n",
    "1. Print out a sample episode\n",
    "\n",
    "The only line you need to change is the call to `Q_learning`, which is where you \n",
    "select the parameters. *Do not change the other cells*. You will be graded according \n",
    "to your implementation; these are checks to make sure it is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode='ansi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only change the following line, to set parameters\n",
    "q_table = Q_learning(env, alpha=.1, gamma=.5, epsilon=.5, episodes=100)\n",
    "display_value_function(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the value function above. Do the numerical values in different states make sense? Why or why not? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[your markdown here]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "evaluate_Q_function(env, q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the evaluate statistics make sense? How do they compare to the random environment below? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[your markdown here]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "frames = sample_episode(env, q_table)\n",
    "print_frames(frames, delay=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.3\n",
    "\n",
    "Now use the function `Q_learning` on the random environment, \n",
    "where `is_slippery=True`. You may want to play around with this a bit \n",
    "to make sure you understand how it differs from the case where `is_slippery=False`.\n",
    "The difference is that there is randomness in the state transitions for each \n",
    "action. If you try to go down, you may go right, for example.\n",
    "\n",
    "As above, you run the following steps:\n",
    "\n",
    "1. Create the environment\n",
    "1. Run Q-learning. This is where you can experiment with different parameters\n",
    "1. Display the value function `v(s) = np.max(q_table[s])`\n",
    "1. Describe the value function, and how the numerical values make sense\n",
    "1. Run `evaluate_Q_function` to get statistics on how well it works\n",
    "1. Comment on the evaluation statistics\n",
    "1. Print out a sample episode\n",
    "\n",
    "The only line you need to change is the call to `Q_learning`, which is where you \n",
    "select the parameters. *Do not change the other cells*. You will be graded according \n",
    "to your implementation; these are checks to make sure it is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change\n",
    "random_env = gym.make(\"FrozenLake-v1\", is_slippery=True, render_mode='ansi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# only change the following line, to set parameters\n",
    "q_table = Q_learning(random_env, alpha=.1, gamma=.5, epsilon=.5, episodes=100)\n",
    "display_value_function(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the value function above. Do the numerical values in different states make sense? Why or why not? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[your markdown here]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "evaluate_Q_function(random_env, q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the output statistics make sense? How do they compare to the deterministic environment?Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[your markdown here]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "frames = sample_episode(random_env, q_table)\n",
    "print_frames(frames, delay=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Add any discussion of your implementation and findings that you wish to share. Nothing specific is required.]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus question\n",
    "\n",
    "What is the common theme of all of the problem names?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
